{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training 128 layer ReZero Transformer on WikiText-2 language modeling\n",
    "\n",
    "In this notebook we will examine how the [ReZero](https://arxiv.org/abs/2003.04887) architecture addition enables or accelerates training in deep [Transformer](https://arxiv.org/pdf/1706.03762.pdf) networks or fully connected networks.\n",
    "\n",
    "The official ReZero repo is [here](https://github.com/majumderb/rezero). Although it is not required for this notebook, you can install ReZero for PyTorch Transformers via `pip install rezero`.\n",
    "\n",
    "Running time of the notebook: 7 minutes on laptop with single RTX 2060 GPU (+ 21 minutes for training 128 layer transformer at the end)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a simple illustration of the power of ReZero we will now train a 128 layer ReZero-Transformer network on a sequence-to-sequence model, and explore why this is not possible with the vanilla Transformer architecture. This discussion is based to the basic PyTorch tutorial [Sequence-to-Sequence Modeling with nn.Transformer and TorchText\n",
    "](https://pytorch.org/tutorials/beginner/transformer_tutorial.html).\n",
    "\n",
    "\n",
    "Before we build the model, let us define the `ReZeroEncoderLayer` following the default PyTorch implementation, but adding a residual weight (default = 0) that by default initializes this layer as the identity map. \n",
    "\n",
    "We also add arguments for pre-residual and post-residual LayerNorm applications. Using post-norm and seting the residual weight to 1 reproduces the standard encoder layer. We will explore the impact of various configurations on signal propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# Import and set manual seed\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.modules.module import Module\n",
    "from torch.nn.modules.activation import MultiheadAttention\n",
    "from torch.nn.modules.container import ModuleList\n",
    "from torch.nn.modules.dropout import Dropout\n",
    "from torch.nn.modules.linear import Linear\n",
    "from torch.nn.modules.normalization import LayerNorm\n",
    "from torch.nn.init import xavier_uniform_\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "######################################################################\n",
    "# Define the ReZero Transformer\n",
    "\n",
    "\n",
    "class ReZeroEncoderLayer(Module):\n",
    "    r\"\"\"ReZero-TransformerEncoderLayer is made up of self-attn and feedforward network.\n",
    "\n",
    "    Args:\n",
    "        d_model: the number of expected features in the input (required).\n",
    "        nhead: the number of heads in the multiheadattention models (required).\n",
    "        dim_feedforward: the dimension of the feedforward network model (default=2048).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        activation: the activation function of intermediate layer, relu or gelu (default=relu).\n",
    "        use_LayerNorm: using either no LayerNorm (dafault=False), or use LayerNorm \"pre\", or \"post\"\n",
    "\n",
    "    Examples::\n",
    "        >>> encoder_layer = ReZeroEncoderLayer(d_model=512, nhead=8)\n",
    "        >>> src = torch.rand(10, 32, 512)\n",
    "        >>> out = encoder_layer(src)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation = \"relu\", \n",
    "                 use_LayerNorm = False, init_resweight = 0, resweight_trainable = True):\n",
    "        super(ReZeroEncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        \n",
    "        # Define the Resisdual Weight for ReZero\n",
    "        self.resweight = torch.nn.Parameter(torch.Tensor([init_resweight]), requires_grad = resweight_trainable)\n",
    "\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = Linear(d_model, dim_feedforward)\n",
    "        self.dropout = Dropout(dropout)\n",
    "        self.linear2 = Linear(dim_feedforward, d_model)\n",
    "        self.use_LayerNorm = use_LayerNorm\n",
    "        if self.use_LayerNorm != False:\n",
    "            self.norm1 = LayerNorm(d_model)\n",
    "            self.norm2 = LayerNorm(d_model)\n",
    "        self.dropout1 = Dropout(dropout)\n",
    "        self.dropout2 = Dropout(dropout)\n",
    "        if activation == \"relu\":\n",
    "            self.activation = F.relu\n",
    "        elif activation == \"gelu\":\n",
    "            self.activation = F.gelu\n",
    "        elif activation == \"tanh\":\n",
    "            self.activation = torch.tanh\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        if 'activation' not in state:\n",
    "            state['activation'] = F.relu\n",
    "        super(ReZeroEncoderLayer, self).__setstate__(state)\n",
    "\n",
    "    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
    "        # type: (Tensor, Optional[Tensor], Optional[Tensor]) -> Tensor\n",
    "        r\"\"\"Pass the input through the encoder layer.\n",
    "        Args:\n",
    "            src: the sequence to the encoder layer (required).\n",
    "            src_mask: the mask for the src sequence (optional).\n",
    "            src_key_padding_mask: the mask for the src keys per batch (optional).\n",
    "        Shape:\n",
    "            see the docs in Transformer class.\n",
    "        \"\"\"\n",
    "        src2 = src\n",
    "        if self.use_LayerNorm == \"pre\":\n",
    "            src2 = self.norm1(src2)\n",
    "        src2 = self.self_attn(src2, src2, src2, attn_mask=src_mask,key_padding_mask=src_key_padding_mask)[0]\n",
    "        # Apply the residual weight to the residual connection. This enables ReZero.\n",
    "        src2 = self.resweight * src2\n",
    "        src2 = self.dropout1(src2)\n",
    "        if self.use_LayerNorm == False:\n",
    "            src = src + src2\n",
    "        elif self.use_LayerNorm == \"pre\":\n",
    "            src = src + src2\n",
    "        elif self.use_LayerNorm == \"post\":\n",
    "            src = self.norm1(src + src2)\n",
    "        src2 = src\n",
    "        if self.use_LayerNorm == \"pre\":\n",
    "            src2 = self.norm1(src2)\n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src2))))\n",
    "        src2 = self.resweight * src2\n",
    "        src2 = self.dropout2(src2)\n",
    "        if self.use_LayerNorm == False:\n",
    "            src = src + src2\n",
    "        elif self.use_LayerNorm == \"pre\":\n",
    "            src = src + src2\n",
    "        elif self.use_LayerNorm == \"post\":\n",
    "            src = self.norm1(src + src2)\n",
    "        return src\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Signal propagation\n",
    "\n",
    "Let us pause and examine the signal propagation properties in a toy deep network `DeepEncoder` by evaluating the singular values of the Transformer input-output Jacobian `io_jacobian_TF`.\n",
    "\n",
    "The entries of the input-output Jacobian matrix reflects the change of each output with respect to each input. The singular value decomposition of this matrix reflects by how much in magnitude (singular value) an input signal (the corresponding singular vector) changes as it propagates through the network, see the [Wikipedia page](https://en.wikipedia.org/wiki/Singular_value_decomposition). A vanishing singular value means that the corresponding singular vector is mapped to zero (poor signal propagation), while a large singular value means that the corresponding singular vector is amplified in magnitude (chaotic signal propagation). Due to these properties, the singular value decomposition provides a useful tool to study signal propagation in neural networks. As we will see in this notebook, singular values close to unity (i.e. dynamical isometry) often coincide with strong training performance.\n",
    "\n",
    "We will compare both pre- and the vanilla (post-) norm variants of the Transformer with the ReZero proposal that eliminates LayerNorm. Since ReZero initializes each layer to perform the identity map by setting all residual weights to zero, we here instead set all the residual weights to 0.1, in order to see a non-trivial distribution of the input-output Jacobian singular values. We define `plot_jacobians` to plot the singular value distributions for each architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jacobian mean squared singular values for  12  layer Transformers:\n",
      "0.810 Vanilla Transformer (post norm)\n",
      "10.242 Transformer with pre-norm\n",
      "1.059 ReZero Transformer (resweight = 0.1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jacobian mean squared singular values for  128  layer Transformers:\n",
      "0.000 Vanilla Transformer (post norm)\n",
      "66.281 Transformer with pre-norm\n",
      "2.142 ReZero Transformer (resweight = 0.1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "########################################################################\n",
    "#Compare io-Jacobian singular values during training of deep Transformer and Deepformer encoders\n",
    "\n",
    "\n",
    "########################################################################\n",
    "#Define input-output Jacobian\n",
    "\n",
    "def io_jacobian_TF(model, x):\n",
    "    le = x.size()[0]\n",
    "    emb = x.size()[2]\n",
    "    noutputs = emb * le\n",
    "    x = x.reshape(noutputs)\n",
    "    x = x.repeat(noutputs,1)\n",
    "    x.requires_grad_(True)\n",
    "    y = model(x.reshape(emb*le,le,emb).transpose(0,1)).reshape(noutputs,-1)\n",
    "    y.backward(torch.eye(noutputs).to(device))\n",
    "    return x.grad.data\n",
    "\n",
    "\n",
    "########################################################################\n",
    "#Define an example deep transformer encoder network\n",
    "\n",
    "class DeepEncoder(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, ninp, nhead, nhid, nlayers, dropout = 0, variant = 'ReZero', \n",
    "                 use_LayerNorm = False, init_resweight = 0):\n",
    "        super(DeepEncoder, self).__init__()\n",
    "        from torch.nn import TransformerEncoder\n",
    "        if variant == 'ReZero':\n",
    "            encoder_layers = ReZeroEncoderLayer(ninp, nhead, nhid, dropout, \n",
    "                                                use_LayerNorm = use_LayerNorm, init_resweight = init_resweight)\n",
    "        else:\n",
    "            encoder_layers = torch.nn.TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self._reset_parameters()\n",
    "        \n",
    "    def forward(self, src):\n",
    "        src = self.transformer_encoder(src)\n",
    "        return src\n",
    "    \n",
    "    def _reset_parameters(self):\n",
    "        r\"\"\"Initiate parameters in the transformer model.\"\"\"\n",
    "        for p in self.parameters():\n",
    "            #print(p.dim()>1)\n",
    "            if p.dim() > 1:\n",
    "                xavier_uniform_(p)\n",
    "            \n",
    "            \n",
    "########################################################################\n",
    "#Define a way to plot histograms of the singular value distributions\n",
    "\n",
    "def plot_jacobians(layers = 128):\n",
    "    d_TF = list()\n",
    "    d_ReZero = list()\n",
    "    d_TF_prenorm = list()\n",
    "\n",
    "    torch.manual_seed(0)\n",
    "    for i in range(10):\n",
    "        src = torch.randn(length,1, emb).to(device)\n",
    "        \n",
    "        model = DeepEncoder(emb, nhead, nhid, layers, dropout, use_LayerNorm='post', init_resweight = 1\n",
    "                           ).to(device)\n",
    "        J = io_jacobian_TF(model,src)\n",
    "        v, d, u = torch.svd(J)\n",
    "        d_TF.append(d.cpu().numpy().tolist())\n",
    "\n",
    "        model = DeepEncoder(emb, nhead, nhid, layers, dropout, use_LayerNorm='pre', init_resweight = 1\n",
    "                           ).to(device)\n",
    "        J = io_jacobian_TF(model,src)\n",
    "        v, d, u = torch.svd(J)\n",
    "        d_TF_prenorm.append(d.cpu().numpy().tolist())\n",
    "\n",
    "        model = DeepEncoder(emb, nhead, nhid, layers, dropout, use_LayerNorm = False, init_resweight = .1\n",
    "                           ).to(device)\n",
    "        J = io_jacobian_TF(model,src)\n",
    "        v, d, u = torch.svd(J)\n",
    "        d_ReZero.append(d.cpu().numpy().tolist())\n",
    "\n",
    "    d_TF = np.asarray(d_TF).flatten()\n",
    "    d_TF_prenorm = np.asarray(d_TF_prenorm).flatten()\n",
    "    d_ReZero = np.asarray(d_ReZero).flatten()\n",
    "\n",
    "    print(\"%0.3f\" % np.mean(d_TF**2),'Vanilla Transformer (post norm)')\n",
    "    print(\"%3.3f\" % np.mean(d_TF_prenorm**2), 'Transformer with pre-norm')\n",
    "    print(\"%0.3f\" % np.mean(d_ReZero**2),'ReZero Transformer (resweight = 0.1)')\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    opacity=.7\n",
    "    plt.ylim((0,1))\n",
    "    plt.xlim((-11,4))\n",
    "    ax.hist(np.log(d_ReZero)/np.log(10), bins = 10, alpha = opacity, label='ReZero', density = True)\n",
    "    ax.hist(np.log(d_TF_prenorm)/np.log(10), bins = 10, alpha = opacity, label='TF prenorm', density = True)\n",
    "    ax.hist(np.log(d_TF)/np.log(10), bins = 10, alpha = opacity, label='TF', density = True)\n",
    "    ax.legend(loc='upper left')\n",
    "    ax.set_xlabel('log (io-Jacobian singular values)')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "########################################################################\n",
    "#Use GPU if available\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "########################################################################\n",
    "#Plot singular value distributions for 12 and 128 layer networks\n",
    "\n",
    "emb = 16\n",
    "length = 4\n",
    "nhead = 4\n",
    "nhid = 64\n",
    "dropout = 0\n",
    "\n",
    "layers = 12\n",
    "print('Jacobian mean squared singular values for ', layers, ' layer Transformers:')\n",
    "plot_jacobians(layers = layers)\n",
    "\n",
    "layers = 128\n",
    "print('Jacobian mean squared singular values for ', layers, ' layer Transformers:')\n",
    "plot_jacobians(layers = layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The blue/orange/green histograms correspond to the Rezero/Prenorm/vanilla Transformer architectures respectively. We observe that while for shallow (12 layer) networks all variants have many singular values close to one (i.e. log(1) = 0 in the figure), for deep networks both the vanilla as well as the prenorm versions have many large/small singular values. The ReZero Transformer maintains singular values much closer to one. We will see below how this affects the training dynamics for deep Transformer networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language modeling\n",
    "\n",
    "We now use each of the three Transformer archtectures defined above to model the WikiText-2 dataset, following the basic PyTorch tutorial PyTorch tutorial [Sequence-to-Sequence Modeling with nn.Transformer and TorchText\n",
    "](https://pytorch.org/tutorials/beginner/transformer_tutorial.html).  The model is tasked to predict which word will follow a sequence of words, and we refer to the tutorial for details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model\n",
    "\n",
    "We now define the `TransformerModel` and several functions that load and prepare the data. Finally, we arrive at the function `setup_and_train`, that defines, trains and evaluates the model, and takes the following parameters as input:\n",
    "\n",
    "`encoder_version` : Defines Transformer architecture: `'ReZero'`, `'pre'`, or `'post'`.\n",
    "\n",
    "`epochs` : Number of epochs to train\n",
    "         \n",
    "`lr` : Learning rate\n",
    "\n",
    "`emsize` : Embedding size\n",
    "\n",
    "`nhid` : Width of feed-forward layers\n",
    "\n",
    "`nlayers` : Number of TransformerEncoder layers\n",
    "\n",
    "`nhead` : Number of self attention heads\n",
    "\n",
    "`dropout` : Dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# Define the model\n",
    "\n",
    "class TransformerModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.1, \n",
    "                 encoder_version = 'ReZero'):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "        self.model_type = 'Transformer'\n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "        if encoder_version == 'ReZero':\n",
    "            encoder_layers = ReZeroEncoderLayer(ninp, nhead, nhid, dropout, \n",
    "                activation = \"relu\", use_LayerNorm = False, init_resweight = 0, \n",
    "                resweight_trainable = True)\n",
    "        elif encoder_version == 'pre':\n",
    "            encoder_layers = ReZeroEncoderLayer(ninp, nhead, nhid, dropout, \n",
    "                activation = \"relu\", use_LayerNorm = 'pre', init_resweight = 1, \n",
    "                resweight_trainable = False)\n",
    "        elif encoder_version == 'post':\n",
    "            encoder_layers = ReZeroEncoderLayer(ninp, nhead, nhid, dropout, \n",
    "                activation = \"relu\", use_LayerNorm = 'post', init_resweight = 1, \n",
    "                resweight_trainable = False)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = torch.nn.Embedding(ntoken, ninp)\n",
    "        self.ninp = ninp\n",
    "        self.decoder = torch.nn.Linear(ninp, ntoken)\n",
    "        self._reset_parameters()\n",
    "        self.init_weights()\n",
    "        \n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "    def _reset_parameters(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                xavier_uniform_(p)\n",
    "\n",
    "    def forward(self, src):\n",
    "        if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "            device = src.device\n",
    "            mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "            self.src_mask = mask\n",
    "\n",
    "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, self.src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "    \n",
    "\n",
    "######################################################################\n",
    "# Positional Encoding\n",
    "\n",
    "class PositionalEncoding(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# Load and batch data\n",
    "\n",
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "TEXT = torchtext.data.Field(tokenize=get_tokenizer(\"basic_english\"),\n",
    "                            init_token='<sos>',\n",
    "                            eos_token='<eos>',\n",
    "                            lower=True)\n",
    "train_txt, val_txt, test_txt = torchtext.datasets.WikiText2.splits(TEXT)\n",
    "TEXT.build_vocab(train_txt)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def batchify(data, bsz):\n",
    "    \n",
    "    data = TEXT.numericalize([data.examples[0].text])\n",
    "    nbatch = data.size(0) // bsz\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "batch_size = 50\n",
    "eval_batch_size = 10\n",
    "train_data = batchify(train_txt, batch_size)\n",
    "val_data = batchify(val_txt, eval_batch_size)\n",
    "test_data = batchify(test_txt, eval_batch_size)\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# get_batch() function generates the input and target sequence for\n",
    "\n",
    "bptt = 35\n",
    "def get_batch(source, i):\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].view(-1)\n",
    "    return data, target\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# setup_and_train function calls, trains and evaluates the model\n",
    "\n",
    "def setup_and_train(epochs, lr, emsize, nhid, nlayers, nhead, dropout, encoder_version, plt_jacobian = True):\n",
    "    \n",
    "    ntokens = len(TEXT.vocab.stoi)  # the size of vocabulary\n",
    "    \n",
    "    ######################################################################\n",
    "    # Model setup\n",
    "\n",
    "    model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, \n",
    "                             dropout, encoder_version = encoder_version).to(device)\n",
    "    model.to(device)\n",
    "\n",
    "\n",
    "    ######################################################################\n",
    "    # Define criterion and optimizer\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adagrad(model.parameters(), lr = lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.9)\n",
    "\n",
    "    ######################################################################\n",
    "    # Define the training\n",
    "\n",
    "    def train():\n",
    "        model.train() # Turn on the train mode\n",
    "        total_loss = 0.\n",
    "        start_time = time.time()\n",
    "        ntokens = len(TEXT.vocab.stoi)\n",
    "        for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "            data, targets = get_batch(train_data, i)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output.view(-1, ntokens), targets)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            log_interval = 200\n",
    "            if batch % log_interval == 0 and batch > 0:\n",
    "                cur_loss = total_loss / log_interval\n",
    "                elapsed = time.time() - start_time\n",
    "                print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                      'lr {:02.2f} | ms/batch {:5.0f} | '\n",
    "                      'loss {:5.2f} | ppl {:6.0f}'.format(\n",
    "                        epoch, batch, len(train_data) // bptt, scheduler.get_lr()[0],\n",
    "                        elapsed * 1000 / log_interval,cur_loss, math.exp(cur_loss)))\n",
    "                total_loss = 0\n",
    "                start_time = time.time()\n",
    "\n",
    "    ######################################################################\n",
    "    # Define the evaluation\n",
    "\n",
    "    def evaluate(eval_model, data_source):\n",
    "        eval_model.eval() # Turn on the evaluation mode\n",
    "        total_loss = 0.\n",
    "        ntokens = len(TEXT.vocab.stoi)\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, data_source.size(0) - 1, bptt):\n",
    "                data, targets = get_batch(data_source, i)\n",
    "                output = eval_model(data)\n",
    "                output_flat = output.view(-1, ntokens)\n",
    "                total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "        return total_loss / (len(data_source) - 1)\n",
    "\n",
    "    ######################################################################\n",
    "    # Train the model\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train()\n",
    "        val_loss = evaluate(model, val_data)\n",
    "        print('-' * 88)\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "              'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                         val_loss, math.exp(val_loss)))\n",
    "        print('-' * 88)\n",
    "        scheduler.step()\n",
    "\n",
    "    ######################################################################\n",
    "    #Plot the spectrum of the io-Jacobian singular values after training\n",
    "    \n",
    "    if plt_jacobian == True:\n",
    "        d_ = list()\n",
    "        for i in range(10):\n",
    "            src = torch.randn(16,1, emsize).to(device)\n",
    "            J = io_jacobian_TF(model.transformer_encoder,src)\n",
    "            v, d, u = torch.svd(J)\n",
    "            d_.append(d.cpu().numpy().tolist())\n",
    "        d_ = np.asarray(d_).flatten()\n",
    "        print('Mean sq singular value of io Jacobian:', \"%0.3f\" % np.mean(d_**2))\n",
    "        fig, ax = plt.subplots()\n",
    "        opacity=.7\n",
    "        plt.ylim((0,1))\n",
    "        plt.xlim((-11,4))\n",
    "        ax.hist(np.log(d_)/np.log(10), bins = 10, alpha = opacity, label='Model.transformer_encoder',\n",
    "                density = True)\n",
    "        ax.legend(loc='upper left')\n",
    "        ax.set_xlabel('log (io-Jacobian singular values)')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and compare three Transformer architectures:\n",
    "\n",
    "We can now easily use the function `setup_and_train` to run experiments by changing between Transformer architectures and modifying hyperparameters.\n",
    "\n",
    "### Vanilla, or post-norm Transformer\n",
    "\n",
    "First, let us use the `'post'` architecture that corresponds to a vanilla Transformer (i.e. we set `resweight = 1` and it is not trainable). Our experiment uses the Adagrad optimizer and no learning-rate warmup. For a 12 layer transformer network we observe slow training.\n",
    "\n",
    "Although the mean squared singular values of the Jacobian remain close to one, the histogram shows a large spread. This indicates that some signals get amplified while others are attenuated, which is associated with poor trainng performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 1192 batches | lr 0.01 | ms/batch    49 | loss  7.01 | ppl   1103\n",
      "| epoch   1 |   400/ 1192 batches | lr 0.01 | ms/batch    49 | loss  6.89 | ppl    981\n",
      "| epoch   1 |   600/ 1192 batches | lr 0.01 | ms/batch    49 | loss  6.89 | ppl    979\n",
      "| epoch   1 |   800/ 1192 batches | lr 0.01 | ms/batch    49 | loss  6.89 | ppl    987\n",
      "| epoch   1 |  1000/ 1192 batches | lr 0.01 | ms/batch    49 | loss  6.89 | ppl    978\n",
      "----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 63.66s | valid loss  6.72 | valid ppl   829.18\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 1192 batches | lr 0.01 | ms/batch    49 | loss  6.80 | ppl    897\n",
      "| epoch   2 |   400/ 1192 batches | lr 0.01 | ms/batch    48 | loss  6.80 | ppl    900\n",
      "| epoch   2 |   600/ 1192 batches | lr 0.01 | ms/batch    48 | loss  6.82 | ppl    915\n",
      "| epoch   2 |   800/ 1192 batches | lr 0.01 | ms/batch    48 | loss  6.84 | ppl    931\n",
      "| epoch   2 |  1000/ 1192 batches | lr 0.01 | ms/batch    48 | loss  6.83 | ppl    928\n",
      "----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 63.01s | valid loss  6.74 | valid ppl   845.14\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 1192 batches | lr 0.01 | ms/batch    48 | loss  6.77 | ppl    873\n",
      "| epoch   3 |   400/ 1192 batches | lr 0.01 | ms/batch    48 | loss  6.78 | ppl    878\n",
      "| epoch   3 |   600/ 1192 batches | lr 0.01 | ms/batch    48 | loss  6.79 | ppl    893\n",
      "| epoch   3 |   800/ 1192 batches | lr 0.01 | ms/batch    48 | loss  6.81 | ppl    911\n",
      "| epoch   3 |  1000/ 1192 batches | lr 0.01 | ms/batch    48 | loss  6.81 | ppl    910\n",
      "----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 63.03s | valid loss  6.76 | valid ppl   864.26\n",
      "----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "# The model is set up with the hyperparameter below.\n",
    "\n",
    "encoder_version = 'post'      # architecture: 'ReZero', 'pre', or 'post' (vanilla)\n",
    "nlayers = 12                     # the number of Layers\n",
    "lr = .01                        # Initial learning rate\n",
    "epochs = 3                      # The number of epochs\n",
    "emsize = 128                    # embedding dimension\n",
    "nhid = 256                      # the dimension of the feedforward network model\n",
    "nhead = 8                       # the number of heads in self attention\n",
    "dropout = 0.1                   # the dropout value\n",
    "\n",
    "setup_and_train(epochs, lr, emsize, nhid, nlayers, nhead, dropout, encoder_version, plt_jacobian = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-norm Transformer\n",
    "\n",
    "Next, let us use the `'pre'` architecture that applies the `LayerNorm` before the residual connection. For the 12 layer Transformer network with otherwise identical hyperparameters we observe faster training.\n",
    "\n",
    "Again, the mean squared singular values of the Jacobian remain close to one, and compared to the 'post' architecture, the histogram shows a smaller spread in the singular values. This coincides with somewhat better trainng performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 1192 batches | lr 0.01 | ms/batch    50 | loss  7.84 | ppl   2536\n",
      "| epoch   1 |   400/ 1192 batches | lr 0.01 | ms/batch    50 | loss  6.04 | ppl    419\n",
      "| epoch   1 |   600/ 1192 batches | lr 0.01 | ms/batch    50 | loss  5.90 | ppl    364\n",
      "| epoch   1 |   800/ 1192 batches | lr 0.01 | ms/batch    50 | loss  5.85 | ppl    348\n",
      "| epoch   1 |  1000/ 1192 batches | lr 0.01 | ms/batch    50 | loss  5.77 | ppl    321\n",
      "----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 65.10s | valid loss  5.49 | valid ppl   243.25\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 1192 batches | lr 0.01 | ms/batch    50 | loss  5.46 | ppl    236\n",
      "| epoch   2 |   400/ 1192 batches | lr 0.01 | ms/batch    50 | loss  5.36 | ppl    214\n",
      "| epoch   2 |   600/ 1192 batches | lr 0.01 | ms/batch    50 | loss  5.36 | ppl    213\n",
      "| epoch   2 |   800/ 1192 batches | lr 0.01 | ms/batch    50 | loss  5.39 | ppl    220\n",
      "| epoch   2 |  1000/ 1192 batches | lr 0.01 | ms/batch    50 | loss  5.37 | ppl    215\n",
      "----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 65.22s | valid loss  5.34 | valid ppl   208.18\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 1192 batches | lr 0.01 | ms/batch    51 | loss  5.19 | ppl    180\n",
      "| epoch   3 |   400/ 1192 batches | lr 0.01 | ms/batch    50 | loss  5.13 | ppl    170\n",
      "| epoch   3 |   600/ 1192 batches | lr 0.01 | ms/batch    50 | loss  5.14 | ppl    171\n",
      "| epoch   3 |   800/ 1192 batches | lr 0.01 | ms/batch    50 | loss  5.20 | ppl    181\n",
      "| epoch   3 |  1000/ 1192 batches | lr 0.01 | ms/batch    50 | loss  5.19 | ppl    179\n",
      "----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 65.33s | valid loss  5.27 | valid ppl   193.95\n",
      "----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "# The model is set up with the hyperparameter below.\n",
    "\n",
    "encoder_version = 'pre'      # architecture: 'ReZero', 'pre', or 'post' (vanilla)\n",
    "nlayers = 12                     # the number of Layers\n",
    "lr = .01                        # Initial learning rate\n",
    "epochs = 3                      # The number of epochs\n",
    "emsize = 128                    # embedding dimension\n",
    "nhid = 256                      # the dimension of the feedforward network model\n",
    "nhead = 8                       # the number of heads in self attention\n",
    "dropout = 0.1                   # the dropout value\n",
    "\n",
    "setup_and_train(epochs, lr, emsize, nhid, nlayers, nhead, dropout, encoder_version, \n",
    "                plt_jacobian = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReZero Transformer\n",
    "\n",
    "Finally, we us use the `'ReZero'` architecture that eliminates the `LayerNorm` but set the residual weight initially to zero, and registers it as a trainable parameter. `ReZero` enables the use of a higher learning rate compared to the other architectures. For the 12 layer Transformer network with otherwise identical hyperparameters we observe the fastest training.\n",
    "\n",
    "\n",
    "The mean squared singular values of the Jacobian are very close to one and the histogram shows a very small spread in the singular values. This coincides with the best trainng performance observed in this comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 1192 batches | lr 0.01 | ms/batch    47 | loss  6.51 | ppl    671\n",
      "| epoch   1 |   400/ 1192 batches | lr 0.01 | ms/batch    47 | loss  5.90 | ppl    365\n",
      "| epoch   1 |   600/ 1192 batches | lr 0.01 | ms/batch    47 | loss  5.77 | ppl    322\n",
      "| epoch   1 |   800/ 1192 batches | lr 0.01 | ms/batch    47 | loss  5.74 | ppl    312\n",
      "| epoch   1 |  1000/ 1192 batches | lr 0.01 | ms/batch    47 | loss  5.67 | ppl    291\n",
      "----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 60.97s | valid loss  5.43 | valid ppl   228.46\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 1192 batches | lr 0.01 | ms/batch    47 | loss  5.29 | ppl    199\n",
      "| epoch   2 |   400/ 1192 batches | lr 0.01 | ms/batch    47 | loss  5.24 | ppl    189\n",
      "| epoch   2 |   600/ 1192 batches | lr 0.01 | ms/batch    47 | loss  5.25 | ppl    190\n",
      "| epoch   2 |   800/ 1192 batches | lr 0.01 | ms/batch    47 | loss  5.29 | ppl    199\n",
      "| epoch   2 |  1000/ 1192 batches | lr 0.01 | ms/batch    47 | loss  5.28 | ppl    196\n",
      "----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 61.27s | valid loss  5.30 | valid ppl   199.76\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 1192 batches | lr 0.01 | ms/batch    48 | loss  5.03 | ppl    153\n",
      "| epoch   3 |   400/ 1192 batches | lr 0.01 | ms/batch    47 | loss  5.02 | ppl    151\n",
      "| epoch   3 |   600/ 1192 batches | lr 0.01 | ms/batch    47 | loss  5.04 | ppl    154\n",
      "| epoch   3 |   800/ 1192 batches | lr 0.01 | ms/batch    47 | loss  5.10 | ppl    165\n",
      "| epoch   3 |  1000/ 1192 batches | lr 0.01 | ms/batch    47 | loss  5.10 | ppl    164\n",
      "----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 61.36s | valid loss  5.23 | valid ppl   186.93\n",
      "----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "# The model is set up with the hyperparameter below.\n",
    "\n",
    "encoder_version = 'ReZero'      # architecture: 'ReZero', 'pre', or 'post' (vanilla)\n",
    "nlayers = 12                     # the number of Layers\n",
    "lr = .01                        # Initial learning rate\n",
    "epochs = 3                      # The number of epochs\n",
    "emsize = 128                    # embedding dimension\n",
    "nhid = 256                      # the dimension of the feedforward network model\n",
    "nhead = 8                       # the number of heads in self attention\n",
    "dropout = 0.1                   # the dropout value\n",
    "\n",
    "setup_and_train(epochs, lr, emsize, nhid, nlayers, nhead, dropout, encoder_version, \n",
    "                plt_jacobian = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 128 layer ReZero Transformer\n",
    "\n",
    "As promised in the title, we can use the `'ReZero'` architecture to train extremely deep Transformer networks. To render a `128` layer transformer tranable, we again reduce the learning rate (to the Adagrad default value of `lr = 0.01`).\n",
    "\n",
    "Training this 128 layer network takes about 20 minutes and after three epochs achieves the best validation ppl of around `168`. \n",
    "\n",
    "Unfortunately, it would require too much memory to quickly evaluate the input-output Jacobian for this deep network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 1192 batches | lr 0.01 | ms/batch   322 | loss  6.89 | ppl    985\n",
      "| epoch   1 |   400/ 1192 batches | lr 0.01 | ms/batch   319 | loss  5.84 | ppl    344\n",
      "| epoch   1 |   600/ 1192 batches | lr 0.01 | ms/batch   328 | loss  5.72 | ppl    304\n",
      "| epoch   1 |   800/ 1192 batches | lr 0.01 | ms/batch   318 | loss  5.68 | ppl    292\n",
      "| epoch   1 |  1000/ 1192 batches | lr 0.01 | ms/batch   324 | loss  5.59 | ppl    269\n",
      "----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 427.25s | valid loss  5.35 | valid ppl   211.38\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 1192 batches | lr 0.01 | ms/batch   320 | loss  5.16 | ppl    174\n",
      "| epoch   2 |   400/ 1192 batches | lr 0.01 | ms/batch   322 | loss  5.12 | ppl    167\n",
      "| epoch   2 |   600/ 1192 batches | lr 0.01 | ms/batch   321 | loss  5.11 | ppl    166\n",
      "| epoch   2 |   800/ 1192 batches | lr 0.01 | ms/batch   319 | loss  5.15 | ppl    173\n",
      "| epoch   2 |  1000/ 1192 batches | lr 0.01 | ms/batch   322 | loss  5.13 | ppl    170\n",
      "----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 422.47s | valid loss  5.19 | valid ppl   179.56\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 1192 batches | lr 0.01 | ms/batch   326 | loss  4.84 | ppl    126\n",
      "| epoch   3 |   400/ 1192 batches | lr 0.01 | ms/batch   320 | loss  4.83 | ppl    126\n",
      "| epoch   3 |   600/ 1192 batches | lr 0.01 | ms/batch   323 | loss  4.85 | ppl    128\n",
      "| epoch   3 |   800/ 1192 batches | lr 0.01 | ms/batch   321 | loss  4.92 | ppl    136\n",
      "| epoch   3 |  1000/ 1192 batches | lr 0.01 | ms/batch   320 | loss  4.91 | ppl    135\n",
      "----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 424.36s | valid loss  5.12 | valid ppl   167.97\n",
      "----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "# The model is set up with the hyperparameter below.\n",
    "\n",
    "encoder_version = 'ReZero'      # architecture: 'ReZero', 'pre', or 'post' (vanilla)\n",
    "nlayers = 128                   # the number of Layers\n",
    "lr = .01                        # Initial learning rate\n",
    "epochs = 3                      # The number of epochs\n",
    "emsize = 128                    # embedding dimension\n",
    "nhid = 256                      # the dimension of the feedforward network model\n",
    "nhead = 8                       # the number of heads in self attention\n",
    "dropout = 0.1                   # the dropout value\n",
    "\n",
    "setup_and_train(epochs, lr, emsize, nhid, nlayers, nhead, dropout, encoder_version, plt_jacobian = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
