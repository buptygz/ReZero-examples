{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training 128 layer ReZero Transformer on WikiText-2 language modeling\n",
    "\n",
    "In this notebook we will examine how the [ReZero](https://arxiv.org/abs/2003.04887) architecture addition enables or accelerates training in deep [Transformer](https://arxiv.org/pdf/1706.03762.pdf) networks or fully connected networks.\n",
    "\n",
    "The official ReZero repo is [here](https://github.com/majumderb/rezero). Although it is not required for this notebook, you can install ReZero for PyTorch Transformers via `pip install rezero`.\n",
    "\n",
    "Running time of the notebook: 7 minutes on laptop with single RTX 2060 GPU (+ 21 minutes for training 128 layer transformer at the end)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a simple illustration of the power of ReZero we will now train a 128 layer ReZero-Transformer network on a sequence-to-sequence model, and explore why this is not possible with the vanilla Transformer architecture. This discussion is based to the basic PyTorch tutorial [Sequence-to-Sequence Modeling with nn.Transformer and TorchText\n",
    "](https://pytorch.org/tutorials/beginner/transformer_tutorial.html).\n",
    "\n",
    "\n",
    "Before we build the model, let us define the `ReZeroEncoderLayer` following the default PyTorch implementation, but adding a residual weight (default = 0) that by default initializes this layer as the identity map. \n",
    "\n",
    "We also add arguments for pre-residual and post-residual LayerNorm applications. Using post-norm and seting the residual weight to 1 reproduces the standard encoder layer. We will explore the impact of various configurations on signal propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# Import and set manual seed\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.modules.module import Module\n",
    "from torch.nn.modules.activation import MultiheadAttention\n",
    "from torch.nn.modules.container import ModuleList\n",
    "from torch.nn.modules.dropout import Dropout\n",
    "from torch.nn.modules.linear import Linear\n",
    "from torch.nn.modules.normalization import LayerNorm\n",
    "from torch.nn.init import xavier_uniform_\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "######################################################################\n",
    "# Define the ReZero Transformer\n",
    "\n",
    "\n",
    "class ReZeroEncoderLayer(Module):\n",
    "    r\"\"\"ReZero-TransformerEncoderLayer is made up of self-attn and feedforward network.\n",
    "\n",
    "    Args:\n",
    "        d_model: the number of expected features in the input (required).\n",
    "        nhead: the number of heads in the multiheadattention models (required).\n",
    "        dim_feedforward: the dimension of the feedforward network model (default=2048).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        activation: the activation function of intermediate layer, relu or gelu (default=relu).\n",
    "        use_LayerNorm: using either no LayerNorm (dafault=False), or use LayerNorm \"pre\", or \"post\"\n",
    "\n",
    "    Examples::\n",
    "        >>> encoder_layer = ReZeroEncoderLayer(d_model=512, nhead=8)\n",
    "        >>> src = torch.rand(10, 32, 512)\n",
    "        >>> out = encoder_layer(src)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation = \"relu\", \n",
    "                 use_LayerNorm = False, init_resweight = 0, resweight_trainable = True):\n",
    "        super(ReZeroEncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        \n",
    "        # Define the Resisdual Weight for ReZero\n",
    "        self.resweight = torch.nn.Parameter(torch.Tensor([init_resweight]), requires_grad = resweight_trainable)\n",
    "\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = Linear(d_model, dim_feedforward)\n",
    "        self.dropout = Dropout(dropout)\n",
    "        self.linear2 = Linear(dim_feedforward, d_model)\n",
    "        self.use_LayerNorm = use_LayerNorm\n",
    "        if self.use_LayerNorm != False:\n",
    "            self.norm1 = LayerNorm(d_model)\n",
    "            self.norm2 = LayerNorm(d_model)\n",
    "        self.dropout1 = Dropout(dropout)\n",
    "        self.dropout2 = Dropout(dropout)\n",
    "        if activation == \"relu\":\n",
    "            self.activation = F.relu\n",
    "        elif activation == \"gelu\":\n",
    "            self.activation = F.gelu\n",
    "        elif activation == \"tanh\":\n",
    "            self.activation = torch.tanh\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        if 'activation' not in state:\n",
    "            state['activation'] = F.relu\n",
    "        super(ReZeroEncoderLayer, self).__setstate__(state)\n",
    "\n",
    "    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
    "        # type: (Tensor, Optional[Tensor], Optional[Tensor]) -> Tensor\n",
    "        r\"\"\"Pass the input through the encoder layer.\n",
    "        Args:\n",
    "            src: the sequence to the encoder layer (required).\n",
    "            src_mask: the mask for the src sequence (optional).\n",
    "            src_key_padding_mask: the mask for the src keys per batch (optional).\n",
    "        Shape:\n",
    "            see the docs in Transformer class.\n",
    "        \"\"\"\n",
    "        src2 = src\n",
    "        if self.use_LayerNorm == \"pre\":\n",
    "            src2 = self.norm1(src2)\n",
    "        src2 = self.self_attn(src2, src2, src2, attn_mask=src_mask,key_padding_mask=src_key_padding_mask)[0]\n",
    "        # Apply the residual weight to the residual connection. This enables ReZero.\n",
    "        src2 = self.resweight * src2\n",
    "        src2 = self.dropout1(src2)\n",
    "        if self.use_LayerNorm == False:\n",
    "            src = src + src2\n",
    "        elif self.use_LayerNorm == \"pre\":\n",
    "            src = src + src2\n",
    "        elif self.use_LayerNorm == \"post\":\n",
    "            src = self.norm1(src + src2)\n",
    "        src2 = src\n",
    "        if self.use_LayerNorm == \"pre\":\n",
    "            src2 = self.norm1(src2)\n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
    "        src2 = self.resweight * src2\n",
    "        src2 = self.dropout2(src2)\n",
    "        if self.use_LayerNorm == False:\n",
    "            src = src + src2\n",
    "        elif self.use_LayerNorm == \"pre\":\n",
    "            src = src + src2\n",
    "        elif self.use_LayerNorm == \"post\":\n",
    "            src = self.norm1(src + src2)\n",
    "        return src\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Signal propagation\n",
    "\n",
    "Let us pause and examine the signal propagation properties in a toy deep network `DeepEncoder` by evaluating the singular values of the Transformer input-output Jacobian `io_jacobian_TF`.\n",
    "\n",
    "The entries of the input-output Jacobian matrix reflects the change of each output with respect to each input. The singular value decomposition of this matrix reflects by how much in magnitude (singular value) an input signal (the corresponding singular vector) changes as it propagates through the network, see the [Wikipedia page](https://en.wikipedia.org/wiki/Singular_value_decomposition). A vanishing singular value means that the corresponding singular vector is mapped to zero (poor signal propagation), while a large singular value means that the corresponding singular vector is amplified in magnitude (chaotic signal propagation). Due to these properties, the singular value decomposition provides a useful tool to study signal propagation in neural networks. As we will see in this notebook, singular values close to unity (i.e. dynamical isometry) often coincide with strong training performance.\n",
    "\n",
    "We will compare both pre- and the vanilla (post-) norm variants of the Transformer with the ReZero proposal that eliminates LayerNorm. Since ReZero initializes each layer to perform the identity map by setting all residual weights to zero, we here instead set all the residual weights to 0.1, in order to see a non-trivial distribution of the input-output Jacobian singular values. We define `plot_jacobians` to plot the singular value distributions for each architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jacobian mean squared singular values for  12  layer Transformers:\n",
      "0.810 Vanilla Transformer (post norm)\n",
      "54.835 Transformer with pre-norm\n",
      "1.059 ReZero Transformer (resweight = 0.1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jacobian mean squared singular values for  128  layer Transformers:\n",
      "0.000 Vanilla Transformer (post norm)\n",
      "870468043243477.875 Transformer with pre-norm\n",
      "2.142 ReZero Transformer (resweight = 0.1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "########################################################################\n",
    "#Compare io-Jacobian singular values during training of deep Transformer and Deepformer encoders\n",
    "\n",
    "\n",
    "########################################################################\n",
    "#Define input-output Jacobian\n",
    "\n",
    "def io_jacobian_TF(model, x):\n",
    "    le = x.size()[0]\n",
    "    emb = x.size()[2]\n",
    "    noutputs = emb * le\n",
    "    x = x.reshape(noutputs)\n",
    "    x = x.repeat(noutputs,1)\n",
    "    x.requires_grad_(True)\n",
    "    y = model(x.reshape(emb*le,le,emb).transpose(0,1)).reshape(noutputs,-1)\n",
    "    y.backward(torch.eye(noutputs).to(device))\n",
    "    return x.grad.data\n",
    "\n",
    "\n",
    "########################################################################\n",
    "#Define an example deep transformer encoder network\n",
    "\n",
    "class DeepEncoder(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, ninp, nhead, nhid, nlayers, dropout = 0, variant = 'ReZero', \n",
    "                 use_LayerNorm = False, init_resweight = 0):\n",
    "        super(DeepEncoder, self).__init__()\n",
    "        from torch.nn import TransformerEncoder\n",
    "        if variant == 'ReZero':\n",
    "            encoder_layers = ReZeroEncoderLayer(ninp, nhead, nhid, dropout, \n",
    "                                                use_LayerNorm = use_LayerNorm, init_resweight = init_resweight)\n",
    "        else:\n",
    "            encoder_layers = torch.nn.TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self._reset_parameters()\n",
    "        \n",
    "    def forward(self, src):\n",
    "        src = self.transformer_encoder(src)\n",
    "        return src\n",
    "    \n",
    "    def _reset_parameters(self):\n",
    "        r\"\"\"Initiate parameters in the transformer model.\"\"\"\n",
    "        for p in self.parameters():\n",
    "            #print(p.dim()>1)\n",
    "            if p.dim() > 1:\n",
    "                xavier_uniform_(p)\n",
    "            \n",
    "            \n",
    "########################################################################\n",
    "#Define a way to plot histograms of the singular value distributions\n",
    "\n",
    "def plot_jacobians(layers = 128):\n",
    "    d_TF = list()\n",
    "    d_ReZero = list()\n",
    "    d_TF_prenorm = list()\n",
    "\n",
    "    torch.manual_seed(0)\n",
    "    for i in range(10):\n",
    "        src = torch.randn(length,1, emb).to(device)\n",
    "        \n",
    "        model = DeepEncoder(emb, nhead, nhid, layers, dropout, use_LayerNorm='post', init_resweight = 1\n",
    "                           ).to(device)\n",
    "        J = io_jacobian_TF(model,src)\n",
    "        v, d, u = torch.svd(J)\n",
    "        d_TF.append(d.cpu().numpy().tolist())\n",
    "\n",
    "        model = DeepEncoder(emb, nhead, nhid, layers, dropout, use_LayerNorm='pre', init_resweight = 1\n",
    "                           ).to(device)\n",
    "        J = io_jacobian_TF(model,src)\n",
    "        v, d, u = torch.svd(J)\n",
    "        d_TF_prenorm.append(d.cpu().numpy().tolist())\n",
    "\n",
    "        model = DeepEncoder(emb, nhead, nhid, layers, dropout, use_LayerNorm = False, init_resweight = .1\n",
    "                           ).to(device)\n",
    "        J = io_jacobian_TF(model,src)\n",
    "        v, d, u = torch.svd(J)\n",
    "        d_ReZero.append(d.cpu().numpy().tolist())\n",
    "\n",
    "    d_TF = np.asarray(d_TF).flatten()\n",
    "    d_TF_prenorm = np.asarray(d_TF_prenorm).flatten()\n",
    "    d_ReZero = np.asarray(d_ReZero).flatten()\n",
    "\n",
    "    print(\"%0.3f\" % np.mean(d_TF**2),'Vanilla Transformer (post norm)')\n",
    "    print(\"%3.3f\" % np.mean(d_TF_prenorm**2), 'Transformer with pre-norm')\n",
    "    print(\"%0.3f\" % np.mean(d_ReZero**2),'ReZero Transformer (resweight = 0.1)')\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    opacity=.7\n",
    "    plt.ylim((0,1))\n",
    "    plt.xlim((-11,4))\n",
    "    ax.hist(np.log(d_ReZero)/np.log(10), bins = 10, alpha = opacity, label='ReZero', density = True)\n",
    "    ax.hist(np.log(d_TF_prenorm)/np.log(10), bins = 10, alpha = opacity, label='TF prenorm', density = True)\n",
    "    ax.hist(np.log(d_TF)/np.log(10), bins = 10, alpha = opacity, label='TF', density = True)\n",
    "    ax.legend(loc='upper left')\n",
    "    ax.set_xlabel('log (io-Jacobian singular values)')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "########################################################################\n",
    "#Use GPU if available\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "########################################################################\n",
    "#Plot singular value distributions for 12 and 128 layer networks\n",
    "\n",
    "emb = 16\n",
    "length = 4\n",
    "nhead = 4\n",
    "nhid = 64\n",
    "dropout = 0\n",
    "\n",
    "layers = 12\n",
    "print('Jacobian mean squared singular values for ', layers, ' layer Transformers:')\n",
    "plot_jacobians(layers = layers)\n",
    "\n",
    "layers = 128\n",
    "print('Jacobian mean squared singular values for ', layers, ' layer Transformers:')\n",
    "plot_jacobians(layers = layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The blue/orange/green histograms correspond to the Rezero/Prenorm/vanilla Transformer architectures respectively. We observe that while for shallow (12 layer) networks all variants have many singular values close to one (i.e. log(1) = 0 in the figure), for deep networks both the vanilla as well as the prenorm versions have many large/small singular values. The ReZero Transformer maintains singular values much closer to one. We will see below how this affects the training dynamics for deep Transformer networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language modeling\n",
    "\n",
    "We now use each of the three Transformer archtectures defined above to model the WikiText-2 dataset, following the basic PyTorch tutorial PyTorch tutorial [Sequence-to-Sequence Modeling with nn.Transformer and TorchText\n",
    "](https://pytorch.org/tutorials/beginner/transformer_tutorial.html).  The model is tasked to predict which word will follow a sequence of words, and we refer to the tutorial for details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model\n",
    "\n",
    "We now define the `TransformerModel` and several functions that load and prepare the data. Finally, we arrive at the function `setup_and_train`, that defines, trains and evaluates the model, and takes the following parameters as input:\n",
    "\n",
    "`encoder_version` : Defines Transformer architecture: `'ReZero'`, `'pre'`, or `'post'`.\n",
    "\n",
    "`epochs` : Number of epochs to train\n",
    "         \n",
    "`lr` : Learning rate\n",
    "\n",
    "`emsize` : Embedding size\n",
    "\n",
    "`nhid` : Width of feed-forward layers\n",
    "\n",
    "`nlayers` : Number of TransformerEncoder layers\n",
    "\n",
    "`nhead` : Number of self attention heads\n",
    "\n",
    "`dropout` : Dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# Define the model\n",
    "\n",
    "class TransformerModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.1, \n",
    "                 encoder_version = 'ReZero'):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "        self.model_type = 'Transformer'\n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "        if encoder_version == 'ReZero':\n",
    "            encoder_layers = ReZeroEncoderLayer(ninp, nhead, nhid, dropout, \n",
    "                activation = \"relu\", use_LayerNorm = False, init_resweight = 0, \n",
    "                resweight_trainable = True)\n",
    "        elif encoder_version == 'pre':\n",
    "            encoder_layers = ReZeroEncoderLayer(ninp, nhead, nhid, dropout, \n",
    "                activation = \"relu\", use_LayerNorm = 'pre', init_resweight = 1, \n",
    "                resweight_trainable = False)\n",
    "        elif encoder_version == 'post':\n",
    "            encoder_layers = ReZeroEncoderLayer(ninp, nhead, nhid, dropout, \n",
    "                activation = \"relu\", use_LayerNorm = 'post', init_resweight = 1, \n",
    "                resweight_trainable = False)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = torch.nn.Embedding(ntoken, ninp)\n",
    "        self.ninp = ninp\n",
    "        self.decoder = torch.nn.Linear(ninp, ntoken)\n",
    "        self._reset_parameters()\n",
    "        self.init_weights()\n",
    "        \n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "    def _reset_parameters(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                xavier_uniform_(p)\n",
    "\n",
    "    def forward(self, src):\n",
    "        if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "            device = src.device\n",
    "            mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "            self.src_mask = mask\n",
    "\n",
    "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, self.src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "    \n",
    "\n",
    "######################################################################\n",
    "# Positional Encoding\n",
    "\n",
    "class PositionalEncoding(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# Load and batch data\n",
    "\n",
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "TEXT = torchtext.data.Field(tokenize=get_tokenizer(\"basic_english\"),\n",
    "                            init_token='<sos>',\n",
    "                            eos_token='<eos>',\n",
    "                            lower=True)\n",
    "train_txt, val_txt, test_txt = torchtext.datasets.WikiText2.splits(TEXT)\n",
    "TEXT.build_vocab(train_txt)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def batchify(data, bsz):\n",
    "    \n",
    "    data = TEXT.numericalize([data.examples[0].text])\n",
    "    nbatch = data.size(0) // bsz\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "batch_size = 50\n",
    "eval_batch_size = 10\n",
    "train_data = batchify(train_txt, batch_size)\n",
    "val_data = batchify(val_txt, eval_batch_size)\n",
    "test_data = batchify(test_txt, eval_batch_size)\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# get_batch() function generates the input and target sequence for\n",
    "\n",
    "bptt = 35\n",
    "def get_batch(source, i):\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].view(-1)\n",
    "    return data, target\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# setup_and_train function calls, trains and evaluates the model\n",
    "\n",
    "def setup_and_train(epochs, lr, emsize, nhid, nlayers, nhead, dropout, encoder_version, plt_jacobian = True):\n",
    "    \n",
    "    ntokens = len(TEXT.vocab.stoi)  # the size of vocabulary\n",
    "    \n",
    "    ######################################################################\n",
    "    # Model setup\n",
    "\n",
    "    model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, \n",
    "                             dropout, encoder_version = encoder_version).to(device)\n",
    "    model.to(device)\n",
    "\n",
    "\n",
    "    ######################################################################\n",
    "    # Define criterion and optimizer\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adagrad(model.parameters(), lr = lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.9)\n",
    "\n",
    "    ######################################################################\n",
    "    # Define the training\n",
    "\n",
    "    def train():\n",
    "        model.train() # Turn on the train mode\n",
    "        total_loss = 0.\n",
    "        start_time = time.time()\n",
    "        ntokens = len(TEXT.vocab.stoi)\n",
    "        for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "            data, targets = get_batch(train_data, i)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output.view(-1, ntokens), targets)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            log_interval = 200\n",
    "            if batch % log_interval == 0 and batch > 0:\n",
    "                cur_loss = total_loss / log_interval\n",
    "                elapsed = time.time() - start_time\n",
    "                print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                      'lr {:02.2f} | ms/batch {:5.0f} | '\n",
    "                      'loss {:5.2f} | ppl {:6.0f}'.format(\n",
    "                        epoch, batch, len(train_data) // bptt, scheduler.get_lr()[0],\n",
    "                        elapsed * 1000 / log_interval,cur_loss, math.exp(cur_loss)))\n",
    "                total_loss = 0\n",
    "                start_time = time.time()\n",
    "\n",
    "    ######################################################################\n",
    "    # Define the evaluation\n",
    "\n",
    "    def evaluate(eval_model, data_source):\n",
    "        eval_model.eval() # Turn on the evaluation mode\n",
    "        total_loss = 0.\n",
    "        ntokens = len(TEXT.vocab.stoi)\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, data_source.size(0) - 1, bptt):\n",
    "                data, targets = get_batch(data_source, i)\n",
    "                output = eval_model(data)\n",
    "                output_flat = output.view(-1, ntokens)\n",
    "                total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "        return total_loss / (len(data_source) - 1)\n",
    "\n",
    "    ######################################################################\n",
    "    # Train the model\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train()\n",
    "        val_loss = evaluate(model, val_data)\n",
    "        print('-' * 88)\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "              'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                         val_loss, math.exp(val_loss)))\n",
    "        print('-' * 88)\n",
    "        scheduler.step()\n",
    "\n",
    "    ######################################################################\n",
    "    #Plot the spectrum of the io-Jacobian singular values after training\n",
    "    \n",
    "    if plt_jacobian == True:\n",
    "        d_ = list()\n",
    "        for i in range(10):\n",
    "            src = torch.randn(16,1, emsize).to(device)\n",
    "            J = io_jacobian_TF(model.transformer_encoder,src)\n",
    "            v, d, u = torch.svd(J)\n",
    "            d_.append(d.cpu().numpy().tolist())\n",
    "        d_ = np.asarray(d_).flatten()\n",
    "        print('Mean sq singular value of io Jacobian:', \"%0.3f\" % np.mean(d_**2))\n",
    "        fig, ax = plt.subplots()\n",
    "        opacity=.7\n",
    "        plt.ylim((0,1))\n",
    "        plt.xlim((-11,4))\n",
    "        ax.hist(np.log(d_)/np.log(10), bins = 10, alpha = opacity, label='Model.transformer_encoder',\n",
    "                density = True)\n",
    "        ax.legend(loc='upper left')\n",
    "        ax.set_xlabel('log (io-Jacobian singular values)')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and compare three Transformer architectures:\n",
    "\n",
    "We can now easily use the function `setup_and_train` to run experiments by changing between Transformer architectures and modifying hyperparameters.\n",
    "\n",
    "### Vanilla, or post-norm Transformer\n",
    "\n",
    "First, let us use the `'post'` architecture that corresponds to a vanilla Transformer (i.e. we set `resweight = 1` and it is not trainable). Our experiment uses the Adagrad optimizer and no learning-rate warmup. For a 6 layer transformer network we observe slow training. After three epochs achieves a validation ppl of around `168`.\n",
    "\n",
    "Although the mean squared singular values of the Jacobian remain close to one, the histogram shows a large spread. This indicates that some signals get amplified while others are attenuated, which is associated with poor trainng performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 1192 batches | lr 0.01 | ms/batch    33 | loss  7.01 | ppl   1106\n",
      "| epoch   1 |   400/ 1192 batches | lr 0.01 | ms/batch    32 | loss  6.89 | ppl    981\n",
      "| epoch   1 |   600/ 1192 batches | lr 0.01 | ms/batch    33 | loss  6.89 | ppl    981\n",
      "| epoch   1 |   800/ 1192 batches | lr 0.01 | ms/batch    33 | loss  6.78 | ppl    884\n",
      "| epoch   1 |  1000/ 1192 batches | lr 0.01 | ms/batch    33 | loss  6.67 | ppl    788\n",
      "----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 41.79s | valid loss  6.40 | valid ppl   599.31\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 1192 batches | lr 0.01 | ms/batch    33 | loss  6.41 | ppl    607\n",
      "| epoch   2 |   400/ 1192 batches | lr 0.01 | ms/batch    33 | loss  6.32 | ppl    558\n",
      "| epoch   2 |   600/ 1192 batches | lr 0.01 | ms/batch    33 | loss  6.26 | ppl    524\n",
      "| epoch   2 |   800/ 1192 batches | lr 0.01 | ms/batch    33 | loss  6.19 | ppl    490\n",
      "| epoch   2 |  1000/ 1192 batches | lr 0.01 | ms/batch    33 | loss  6.09 | ppl    442\n",
      "----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 42.74s | valid loss  5.87 | valid ppl   352.86\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 1192 batches | lr 0.01 | ms/batch    33 | loss  5.91 | ppl    370\n",
      "| epoch   3 |   400/ 1192 batches | lr 0.01 | ms/batch    33 | loss  5.85 | ppl    347\n",
      "| epoch   3 |   600/ 1192 batches | lr 0.01 | ms/batch    33 | loss  5.83 | ppl    340\n",
      "| epoch   3 |   800/ 1192 batches | lr 0.01 | ms/batch    33 | loss  5.84 | ppl    344\n",
      "| epoch   3 |  1000/ 1192 batches | lr 0.01 | ms/batch    33 | loss  5.79 | ppl    327\n",
      "----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 42.60s | valid loss  5.67 | valid ppl   291.00\n",
      "----------------------------------------------------------------------------------------\n",
      "Mean sq singular value of io Jacobian: 1.561\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "######################################################################\n",
    "# The model is set up with the hyperparameter below.\n",
    "\n",
    "encoder_version = 'post'      # architecture: 'ReZero', 'pre', or 'post' (vanilla)\n",
    "nlayers = 6                     # the number of Layers\n",
    "lr = .01                        # Initial learning rate\n",
    "epochs = 3                      # The number of epochs\n",
    "emsize = 128                    # embedding dimension\n",
    "nhid = 256                      # the dimension of the feedforward network model\n",
    "nhead = 8                       # the number of heads in self attention\n",
    "dropout = 0.1                   # the dropout value\n",
    "\n",
    "setup_and_train(epochs, lr, emsize, nhid, nlayers, nhead, dropout, encoder_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla, or post-norm Transformer\n",
    "\n",
    "Next, let us use the `'pre'` architecture that applies the `LayerNorm` before the residual connection. For the 6 layer Transformer network with otherwise identical hyperparameters we observe faster training.\n",
    "\n",
    "Again, the mean squared singular values of the Jacobian remain close to one, and compared to the 'post' architecture, the histogram shows a smaller spread in the singular values. This coincides with somewhat better trainng performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 1192 batches | lr 0.01 | ms/batch    34 | loss  7.37 | ppl   1583\n",
      "| epoch   1 |   400/ 1192 batches | lr 0.01 | ms/batch    33 | loss  6.02 | ppl    410\n",
      "| epoch   1 |   600/ 1192 batches | lr 0.01 | ms/batch    33 | loss  5.89 | ppl    363\n",
      "| epoch   1 |   800/ 1192 batches | lr 0.01 | ms/batch    33 | loss  5.85 | ppl    348\n",
      "| epoch   1 |  1000/ 1192 batches | lr 0.01 | ms/batch    33 | loss  5.78 | ppl    324\n",
      "----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 42.67s | valid loss  5.50 | valid ppl   244.16\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 1192 batches | lr 0.01 | ms/batch    33 | loss  5.44 | ppl    231\n",
      "| epoch   2 |   400/ 1192 batches | lr 0.01 | ms/batch    33 | loss  5.37 | ppl    215\n",
      "| epoch   2 |   600/ 1192 batches | lr 0.01 | ms/batch    33 | loss  5.37 | ppl    216\n",
      "| epoch   2 |   800/ 1192 batches | lr 0.01 | ms/batch    33 | loss  5.41 | ppl    224\n",
      "| epoch   2 |  1000/ 1192 batches | lr 0.01 | ms/batch    33 | loss  5.39 | ppl    220\n",
      "----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 42.70s | valid loss  5.35 | valid ppl   210.36\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 1192 batches | lr 0.01 | ms/batch    34 | loss  5.20 | ppl    181\n",
      "| epoch   3 |   400/ 1192 batches | lr 0.01 | ms/batch    33 | loss  5.16 | ppl    173\n",
      "| epoch   3 |   600/ 1192 batches | lr 0.01 | ms/batch    33 | loss  5.17 | ppl    175\n",
      "| epoch   3 |   800/ 1192 batches | lr 0.01 | ms/batch    33 | loss  5.23 | ppl    186\n",
      "| epoch   3 |  1000/ 1192 batches | lr 0.01 | ms/batch    33 | loss  5.21 | ppl    184\n",
      "----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 42.68s | valid loss  5.28 | valid ppl   196.01\n",
      "----------------------------------------------------------------------------------------\n",
      "Mean sq singular value of io Jacobian: 2.369\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "######################################################################\n",
    "# The model is set up with the hyperparameter below.\n",
    "\n",
    "encoder_version = 'pre'      # architecture: 'ReZero', 'pre', or 'post' (vanilla)\n",
    "nlayers = 6                     # the number of Layers\n",
    "lr = .01                        # Initial learning rate\n",
    "epochs = 3                      # The number of epochs\n",
    "emsize = 128                    # embedding dimension\n",
    "nhid = 256                      # the dimension of the feedforward network model\n",
    "nhead = 8                       # the number of heads in self attention\n",
    "dropout = 0.1                   # the dropout value\n",
    "\n",
    "setup_and_train(epochs, lr, emsize, nhid, nlayers, nhead, dropout, encoder_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReZero Transformer\n",
    "\n",
    "Finally, we us use the `'ReZero'` architecture that eliminates the `LayerNorm` but set the residual weight initially to zero, and registers it as a trainable parameter. `ReZero` enables the use of a higher learning rate compared to the other architectures. For the 6 layer Transformer network with otherwise identical hyperparameters we observe the fastest training.\n",
    "\n",
    "\n",
    "The mean squared singular values of the Jacobian are very close to one and the histogram shows a very small spread in the singular values. This coincides with the best trainng performance observed in this comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 1192 batches | lr 0.04 | ms/batch    32 | loss  7.03 | ppl   1134\n",
      "| epoch   1 |   400/ 1192 batches | lr 0.04 | ms/batch    32 | loss  5.69 | ppl    297\n",
      "| epoch   1 |   600/ 1192 batches | lr 0.04 | ms/batch    32 | loss  5.58 | ppl    264\n",
      "| epoch   1 |   800/ 1192 batches | lr 0.04 | ms/batch    32 | loss  5.55 | ppl    256\n",
      "| epoch   1 |  1000/ 1192 batches | lr 0.04 | ms/batch    32 | loss  5.47 | ppl    238\n",
      "----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 41.55s | valid loss  5.33 | valid ppl   205.66\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 1192 batches | lr 0.03 | ms/batch    33 | loss  4.90 | ppl    135\n",
      "| epoch   2 |   400/ 1192 batches | lr 0.03 | ms/batch    32 | loss  4.86 | ppl    129\n",
      "| epoch   2 |   600/ 1192 batches | lr 0.03 | ms/batch    32 | loss  4.86 | ppl    129\n",
      "| epoch   2 |   800/ 1192 batches | lr 0.03 | ms/batch    32 | loss  4.93 | ppl    139\n",
      "| epoch   2 |  1000/ 1192 batches | lr 0.03 | ms/batch    32 | loss  4.93 | ppl    138\n",
      "----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 41.56s | valid loss  5.20 | valid ppl   181.99\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 1192 batches | lr 0.03 | ms/batch    33 | loss  4.59 | ppl     99\n",
      "| epoch   3 |   400/ 1192 batches | lr 0.03 | ms/batch    32 | loss  4.56 | ppl     96\n",
      "| epoch   3 |   600/ 1192 batches | lr 0.03 | ms/batch    32 | loss  4.57 | ppl     97\n",
      "| epoch   3 |   800/ 1192 batches | lr 0.03 | ms/batch    32 | loss  4.67 | ppl    107\n",
      "| epoch   3 |  1000/ 1192 batches | lr 0.03 | ms/batch    32 | loss  4.67 | ppl    107\n",
      "----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 41.61s | valid loss  5.16 | valid ppl   173.96\n",
      "----------------------------------------------------------------------------------------\n",
      "Mean sq singular value of io Jacobian: 1.089\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEKCAYAAADpfBXhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAcIElEQVR4nO3de5xVdb3/8ddbvFCImIL9CijohCbCIHJRH14SLQRT5JAkdPF6QFPTNG8d09CyTmp59/jzqFGJioYlD6W0OpDdBIbAEUSLEGMOnp8EHo+oqOjn98dajJthz+w9wx725sv7+XjwYK/v+q61Pmvvmfesvdbe36WIwMzM0rJDtQswM7PKc7ibmSXI4W5mliCHu5lZghzuZmYJcribmSWoZLhLulvSS5IWtzBfkm6StExSg6QDKl+mmZm1RTlH7lOBUa3MHw30y/9NBv59y8syM7MtUTLcI+IJYG0rXY4HfhyZJ4HdJX2oUgWamVnb7ViBdfQEVhZMN+ZtLzbvKGky2dE9Xbp0GfKJT3yiAps3s61pxZrX2r1snz27VLCS7dOCBQv+ERE9SvWrRLirSFvRMQ0i4g7gDoChQ4dGfX19BTZvZlvT6VPnt3vZu04ZVsFKtk+SXiinXyU+LdMI9C6Y7gWsqsB6zcysnSoR7jOBk/JPzRwEvBIRm52SMTOzrafkaRlJ9wFHAN0lNQLfBHYCiIjbgVnAMcAy4HXg1I4q1szMylMy3CNiYon5AZxdiWLefvttGhsbWb9+fSVWZ9bhOnfuTK9evdhpp52qXYrZJipxQbViGhsb6dq1K3369EEqdp3WrHZEBGvWrKGxsZG+fftWuxyzTdTU8APr169nzz33dLDbNkESe+65p99pWk2qqXAHHOy2TfHPq9Wqmgt3MzPbcjV1zr25LfmyRDH+AoWZbS985N6MJL70pS81TW/YsIEePXpw7LHHtmk9ffr04R//+Ee7+txwww28/vrrbdpee9x0003su+++fOELX+jwbdWycl4rs22Nw72ZLl26sHjxYt544w0AfvWrX9GzZ8+tWkNr4f7OO+9UbDu33XYbs2bNYtq0aWX137BhQ8W23RHr21oq+RqYdRSHexGjR4/m0UcfBeC+++5j4sT3Puq/du1axo4dS11dHQcddBANDQ0ArFmzhpEjRzJ48GDOOOMMso//Z+655x6GDx/O/vvvzxlnnNFqONx0002sWrWKESNGMGLECAB23XVXrrjiCg488ED+9Kc/cdVVVzFs2DAGDBjA5MmTm7Z1xBFHcMkllzB8+HD23ntvfve73wGwZMmSpu3X1dXx17/+lTPPPJPly5czZswYrr/++hb3a8qUKUyePJmRI0dy0kknMXXqVMaOHctxxx1H3759ueWWW/jBD37A4MGDOeigg1i7NhtA9G9/+xujRo1iyJAhHHbYYTz77LMAnHLKKVxwwQWMGDGCSy65pOhz8Nprr3HaaacxbNgwBg8ezMMPPwzA1KlTGTduHKNGjaJfv35cfPHFTcv88pe/5IADDmDQoEEcddRRFX+tmr8GZrXO4V7EhAkTuP/++1m/fj0NDQ0ceOCBTfO++c1vMnjwYBoaGvjOd77DSSedBMCVV17JoYceysKFCxkzZgx///vfAVi6dCnTp0/nD3/4A4sWLaJTp06tHimfe+65fPjDH2b27NnMnj0byMJuwIABzJ07l0MPPZRzzjmH+fPnN73DeOSRR5qW37BhA/PmzeOGG27gyiuvBOD222/nvPPOY9GiRdTX19OrVy9uv/32pu2cf/75Le4XwIIFC3j44Ye59957AVi8eDH33nsv8+bN47LLLuP9738/Cxcu5OCDD+bHP/4xAJMnT+bmm29mwYIFXHfddZx11llN6/vLX/7Cr3/9a77//e8XfQ6uvvpqjjzySObPn8/s2bO56KKLeO21bCTCRYsWMX36dJ5++mmmT5/OypUrWb16NZMmTWLGjBk89dRTPPjggxV/rZq/Bma1rqYvqFZLXV0dK1as4L777uOYY47ZZN7vf/97ZsyYAcCRRx7JmjVreOWVV3jiiSd46KGHAPjMZz7DBz7wAQB+85vfsGDBAoYNyy7mvvHGG+y1115tqqdTp0589rOfbZqePXs211xzDa+//jpr165lv/3247jjjgNg3LhxAAwZMoQVK1YAcPDBB3P11VfT2NjIuHHj6Nev32bbaGm/AMaMGcP73ve+pr4jRoyga9eudO3alW7dujVte+DAgTQ0NLBu3Tr++Mc/Mn78+KZl3nzzzabH48ePp1OnTi3u7+OPP87MmTO57rrrgOz7DxsD+KijjqJbt24A9O/fnxdeeIGXX36Zww8/vOmLRHvssUer+9Se16r5a2BW6xzuLRgzZgwXXnghc+bMYc2aNU3thW/hN9r4Wedin3mOCE4++WS++93vtruWzp07N4Xh+vXrOeuss6ivr6d3795MmTJlky/R7LLLLkAWRhvPaX/+85/nwAMP5NFHH+Xoo4/mzjvv5Mgjj9yszpb2q0uXTcfg3rgNgB122KFpeocddmDDhg28++677L777ixatKjo/jRfX3MRwYwZM9hnn302aZ87d+4m2964jxHR4nPf0j619bUqfA3MtgU1He7V/OjiaaedRrdu3Rg4cCBz5sxpaj/88MOZNm0al19+OXPmzKF79+7stttuTe3f+MY3+MUvfsHLL78MZEeaxx9/POeffz577bUXa9eu5dVXX+WjH/1oi9vu2rUrr776Kt27d99s3sYg7969O+vWreOnP/0pJ5xwQqv7snz5cj72sY9x7rnnsnz5choaGjYL95b2qz122203+vbty4MPPsj48eOJCBoaGhg0aFBZyx999NHcfPPN3HzzzUhi4cKFDB48uMX+Bx98MGeffTbPP/88ffv2Ze3ateyxxx5b5bUyq1U+596CXr16cd55523WPmXKFOrr66mrq+PSSy/lRz/6EZCd333iiSc44IADePzxx/nIRz4CZKcOvv3tbzNy5Ejq6ur49Kc/zYsvbj4i8jHHHMOqVdkw+JMnT2b06NFNF1QL7b777kyaNImBAwcyduzYplMIrZk+fToDBgxg//3359lnn93kfHqp/WqvadOmcddddzFo0CD222+/poui5bj88st5++23qaurY8CAAVx++eWt9u/Rowd33HEH48aNY9CgQZx44omt7tOWvlZm2wIVe+u6NRS7E9PSpUvZd999q1KPWXttbz+3vhNTdUlaEBFDS/XzkbuZWYJq+py7pe2HP/whN9544yZthxxyCLfeemuVKjJLR82Fe0uffLD0nHrqqZx66rZ9465qndY0K6WmTst07tyZNWvW+BfGtgkbb9bRuXPnapditpmaOnLv1asXjY2NrF69utqlmJVl4232zGpNTYX7Tjvt5NuVmZlVQE2dljEzs8pwuJuZJcjhbmaWIIe7mVmCHO5mZglyuJuZJcjhbmaWIIe7mVmCHO5mZglyuJuZJcjhbmaWIIe7mVmCHO5mZglyuJuZJcjhbmaWoLLCXdIoSc9JWibp0iLzPyJptqSFkhokHVP5Us3MrFwlw11SJ+BWYDTQH5goqX+zbt8AHoiIwcAE4LZKF2pmZuUr58h9OLAsIpZHxFvA/cDxzfoEsFv+uBuwqnIlmplZW5UT7j2BlQXTjXlboSnAFyU1ArOArxRbkaTJkuol1fs+qWZmHaeccFeRtmg2PRGYGhG9gGOAn0jabN0RcUdEDI2IoT169Gh7tWZmVpZywr0R6F0w3YvNT7ucDjwAEBF/AjoD3StRoJmZtV054T4f6Cepr6SdyS6YzmzW5+/AUQCS9iULd593MTOrkpLhHhEbgHOAx4ClZJ+KWSLpKklj8m5fAyZJegq4DzglIpqfujEzs61kx3I6RcQssgulhW1XFDx+BjiksqWZmVl7+RuqZmYJcribmSXI4W5mliCHu5lZghzuZmYJcribmSXI4W5mliCHu5lZghzuZmYJcribmSXI4W5mliCHu5lZghzuZmYJcribmSXI4W5mliCHu5lZghzuZmYJcribmSXI4W5mliCHu5lZghzuZmYJcribmSXI4W5mliCHu5lZghzuZmYJcribmSXI4W5mliCHu5lZghzuZmYJcribmSXI4W5mliCHu5lZghzuZmYJcribmSWorHCXNErSc5KWSbq0hT6fk/SMpCWS7q1smWZm1hY7luogqRNwK/BpoBGYL2lmRDxT0Kcf8HXgkIh4WdJeHVWwmZmVVs6R+3BgWUQsj4i3gPuB45v1mQTcGhEvA0TES5Ut08zM2qKccO8JrCyYbszbCu0N7C3pD5KelDSq2IokTZZUL6l+9erV7avYzMxKKifcVaQtmk3vCPQDjgAmAndK2n2zhSLuiIihETG0R48eba3VzMzKVE64NwK9C6Z7AauK9Hk4It6OiOeB58jC3szMqqCccJ8P9JPUV9LOwARgZrM+PwdGAEjqTnaaZnklCzUzs/KVDPeI2ACcAzwGLAUeiIglkq6SNCbv9hiwRtIzwGzgoohY01FFm5lZ60p+FBIgImYBs5q1XVHwOIAL8n9mZlZl/oaqmVmCHO5mZglyuJuZJcjhbmaWIIe7mVmCHO5mZglyuJuZJcjhbmaWIIe7mVmCHO5mZglyuJuZJcjhbmaWIIe7mVmCHO5mZglyuJuZJcjhbmaWIIe7mVmCHO5mZglyuJuZJcjhbmaWIIe7mVmCHO5mZglyuJuZJcjhbmaWIIe7mVmCHO5mZglyuJuZJcjhbmaWIIe7mVmCHO5mZglyuJuZJcjhbmaWIIe7mVmCHO5mZglyuJuZJaiscJc0StJzkpZJurSVfidICklDK1eimZm1Vclwl9QJuBUYDfQHJkrqX6RfV+BcYG6lizQzs7Yp58h9OLAsIpZHxFvA/cDxRfp9C7gGWF/B+szMrB3KCfeewMqC6ca8rYmkwUDviHiktRVJmiypXlL96tWr21ysmZmVp5xwV5G2aJop7QBcD3yt1Ioi4o6IGBoRQ3v06FF+lWZm1iblhHsj0LtguhewqmC6KzAAmCNpBXAQMNMXVc3MqqeccJ8P9JPUV9LOwARg5saZEfFKRHSPiD4R0Qd4EhgTEfUdUrGZmZVUMtwjYgNwDvAYsBR4ICKWSLpK0piOLtDMzNpux3I6RcQsYFaztita6HvElpdlZmZbwt9QNTNLkMPdzCxBDnczswQ53M3MEuRwNzNLkMPdzCxBDnczswQ53M3MEuRwNzNLkMPdzCxBDnczswQ53M3MEuRwNzNLkMPdzCxBDnczswQ53M3MEuRwNzNLkMPdzCxBDnczswQ53M3MEuRwNzNLkMPdzCxBDnczswQ53M3MEuRwNzNLkMPdzCxBDnczswQ53M3MEuRwNzNLkMPdzCxBDnczswQ53M3MEuRwNzNLkMPdzCxBZYW7pFGSnpO0TNKlReZfIOkZSQ2SfiPpo5Uv1czMylUy3CV1Am4FRgP9gYmS+jfrthAYGhF1wE+BaypdqJmZla+cI/fhwLKIWB4RbwH3A8cXdoiI2RHxej75JNCrsmWamVlblBPuPYGVBdONeVtLTgd+UWyGpMmS6iXVr169uvwqzcysTcoJdxVpi6IdpS8CQ4Fri82PiDsiYmhEDO3Ro0f5VZqZWZvsWEafRqB3wXQvYFXzTpI+BVwGfDIi3qxMeWZm1h7lHLnPB/pJ6itpZ2ACMLOwg6TBwP8FxkTES5Uv08zM2qJkuEfEBuAc4DFgKfBARCyRdJWkMXm3a4FdgQclLZI0s4XVmZnZVlDOaRkiYhYwq1nbFQWPP1XhuszMbAv4G6pmZglyuJuZJcjhbmaWIIe7mVmCHO5mZglyuJuZJcjhbmaWIIe7mVmCHO5mZglyuJuZJcjhbmaWoLLGljGztJw+dX61S7AO5iN3M7MEOdzNzBLkcDczS5DD3cwsQQ53M7MEOdzNzBLkcDczS5DD3cwsQQ53M7MEOdzNzBLkcDczS5DD3cwsQQ53M7MEOdzNzBLkcDczS5DD3cwsQQ53M7MEOdzNzBLkcDczS5DD3cwsQQ53M7MEOdzNzBK0Y7ULMLPtx+lT57drubtOGVbhStJX1pG7pFGSnpO0TNKlRebvIml6Pn+upD6VLtTMzMpXMtwldQJuBUYD/YGJkvo363Y68HJEfBy4HvhepQs1M7PylXNaZjiwLCKWA0i6HzgeeKagz/HAlPzxT4FbJCkiooK1mpXU3rf91bIlpxu2tX21rauccO8JrCyYbgQObKlPRGyQ9AqwJ/CPwk6SJgOT88l1kp5rT9EldG++3W2Aa+54NVnv3ae2Orsmay6hQ2ou8TxtiW3xOd6nnE7lhLuKtDU/Ii+nDxFxB3BHGdtsN0n1ETG0I7dRaa65421r9YJr3hq2tXohq7mcfuVcUG0EehdM9wJWtdRH0o5AN2BtOQWYmVnllRPu84F+kvpK2hmYAMxs1mcmcHL++ATgP32+3cysekqelsnPoZ8DPAZ0Au6OiCWSrgLqI2ImcBfwE0nLyI7YJ3Rk0SV06GmfDuKaO962Vi+45q1hW6sXyqxZPsA2M0uPhx8wM0uQw93MLEHJhLuk8ZKWSHpX0tBm876eD43wnKSjq1VjayTtL+lJSYsk1UsaXu2aSpH0lfw5XSLpmmrXUy5JF0oKSd2rXUspkq6V9KykBkk/k7R7tWsqptQQJbVGUm9JsyUtzX9+z6t2TeWQ1EnSQkmPlOqbTLgDi4FxwBOFjflQCROA/YBRwG35kAq15hrgyojYH7gin65ZkkaQfTO5LiL2A66rckllkdQb+DTw92rXUqZfAQMiog74C/D1KtezmTKHKKk1G4CvRcS+wEHA2dtAzQDnAUvL6ZhMuEfE0ogo9o3X44H7I+LNiHgeWEY2pEKtCWC3/HE3Nv8uQa35MvBvEfEmQES8VOV6ynU9cDFFvmRXiyLi8YjYkE8+SfY9k1rTNERJRLwFbByipGZFxIsR8ef88atkgdmzulW1TlIv4DPAneX0TybcW1Fs+IRafBG/ClwraSXZUXDNHaE1szdwWD4K6G8l1fyYrJLGAP8VEU9Vu5Z2Og34RbWLKGJb+R0rKh/FdjAwt7qVlHQD2YHJu+V03qbGc5f0a+D/FJl1WUQ83NJiRdqqctTWWv3AUcD5ETFD0ufIvjvwqa1ZX3Ml6t0R+ADZW9phwAOSPlbtL6+VqPlfgZFbt6LSyvm5lnQZ2amEaVuztjLVzO9YW0naFZgBfDUi/rfa9bRE0rHASxGxQNIR5SyzTYV7RLQn7MoZPmGraK1+ST8mO58G8CBlvvXqSCXq/TLwUB7m8yS9SzYI0+qtVV8xLdUsaSDQF3hKEmQ/B3+WNDwi/nsrlriZUj/Xkk4GjgWOqvYfzxbUzO9YW0jaiSzYp0XEQ9Wup4RDgDGSjgE6A7tJuicivtjSAtvDaZmZwIT8hiJ9gX7AvCrXVMwq4JP54yOBv1axlnL8nKxOJO0N7EwNj64XEU9HxF4R0Sci+pAF0gHVDvZSJI0CLgHGRMTr1a6nBeUMUVJTlP2FvwtYGhE/qHY9pUTE1yOiV/6zO4FsiJcWgx22sSP31kj6Z+BmoAfwqKRFEXF0PlTCA2Tjz28Azo6Id6pZawsmATfmA6+t572hkWvV3cDdkhYDbwEn1+hR5bbuFmAX4Ff5O44nI+LM6pa0qZaGKKlyWaUcAnwJeFrSorztXyNiVhVrqigPP2BmlqDt4bSMmdl2x+FuZpYgh7uZWYIc7mZmCXK4m5klyOGeKEnrKriuGyQdnj++sy0DLEk6opwR7NpYz4piIzpKOlPSSZXcVrP1t2nf27DeOc1HMu1IkqZIurDC6/y1pA9Ucp22ZZL5nLt1DEl7AAdFxFcBIuJfqlxSiyLi9g5ef03su6RONfhdjZ8AZwFXV7sQy/jIPXHKXCtpsaSnJZ2Yt+8g6bZ8LOtHJM2SdEKRVZwA/LJgfU1HmZIm5utcLOl7ZdQyXNIf8/Go/yhpn7y9k6Tr8nU1SPpK3n5U3vdpSXdL2qVgdRdJmpf/+3jev+mIVNIkSfMlPSVphqT35+1TJd2Ub395sX2W1EXSo/myiwues8J9Xyfp6rzPk5I+mLf/Uz49X9JVG99BNX8HI+kWSacU2fa/KxvPf4mkKwvaV0i6QtLvgfEF7d3yeTvk0++XtFLSTi09B822V7hP3SWtKHhNrs2Xb5B0Rt7+IUlPKLvvwGJJh+WrmglMbPnVt63N4Z6+ccD+wCCygciulfShvL0PMBD4F+DgFpY/BFjQvFHSh4HvkQ1BsD8wTNLYErU8CxweEYPJxqz/Tt4+mWzcl8H5uOXTJHUGpgInRsRAsneZXy5Y1/9GxHCyb3DeUGRbD0XEsIgYRDac6+kF8z4EHEo2Xsu/FVl2FLAqIgZFxAAK/rgV6EL2bdFBZPcQmJS33wjcGBHDaN/4KpdFxFCgDvikpLqCeesj4tCIuH9jQ0S8AjzFe0NXHAc8FhFv0/pzUMrpwCv5fgwDJikbvuPz+fo3/kwtyut4GdhF0p7t2GfrAA739B0K3BcR70TE/wN+S/bLeijwYES8m4+vMruF5T9E8cHAhgFzImJ1Pt74NODwErV0Ax5UNmTB9WQ3UIHsj87tG8ctj4i1wD7A8xHxl7zPj5qt/76C/4v9YRog6XeSnga+ULAtgJ/n+/0M8MEiyz4NfErS9yQdlgdoc28BG4/EF5D9oSSv5cH88b1Flivlc5L+DCzMay48xz+9hWWmAyfmjycU9GvtOShlJHCSsq/mzwX2JBuXaT5wqqQpwMB8LPSNXgI+3IZtWAdyuKev2HCsrbU39wbZKHRlLS/pn/O37Iu0+UXCbwGz86Ph4wrWKzYfIrZUfdHC442mAufkR/1Xsuk+vNnadvI/KEPIQv67kq4osv63C8bSeYfS1682sOnv22bPaX5kfCHZ6I91wKPN+r3WwrpnAqOVXR8ZAvxn3j6Vlp+DYnUVzhfwlYjYP//XN79xyBNkf2T/C/iJNr2A3Zns58VqgMM9fU8AJ+bnUHuQ/WLOA34PfDY/9/5B4IgWll8KfLxI+1yy0wbdld1mbSLw24j4WUEg1DdbphtZKACcUtD+OHCmskHTNl7EfRbos/F8OtkgT78tWObEgv//VKS+rsCLyoZ1/UIL+1ZUfsrp9Yi4h+zGKQe0YfEngc/mjycUtL8A9Fc2Omk3svH7m9uNLMBfyV+T0eVsMCLWkb2mNwKPFFxsLec5WEH2BwGy6ysbPQZ8OV8WSXvn1yI+Sjau+H+Qjap4QD5fZGPSryinZut4/rRM+n5GdqrgKbIj3Isj4r8lzSALmMVk9+acCxQ7/fAocAbNxpePiBclfZ3sdI6AWS3cMGVH3jtSvgb4kaQLeO/oknzdewMNkt4G/iMibpF0KtlpnB3JTgcUfhpmF0lzyQ5Qil3IuzzfpxfIjsC7FunTkoFk1ybeBd5m03P9pXwVuEfS18ieu1cAImKlstFJG8iGc17YfMGIeErSQmAJsBz4Qxu2O53sdNARBW3lPAfXkd1o5Uts/pr0IRvzXmSn5sbm678of53WARuP3IeQXYPYgNUEjwq5HZO0a0Ssyy+CzQMOKTa+ef4JjWMj4n/asY3zgJ4RcfGWV1z78k+kvBERIWkCMDEiavp+opUg6UZgZkT8ptq1WMZH7tu3RyTtTnajjW+1cuOKrwEfAdoU7pLuAgYAn9uiKrctQ4Bb8qPd/yG77+n2YLGDvbb4yN3MLEG+oGpmliCHu5lZghzuZmYJcribmSXI4W5mlqD/D8mP+EfGQRZ7AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "######################################################################\n",
    "# The model is set up with the hyperparameter below.\n",
    "\n",
    "encoder_version = 'ReZero'      # architecture: 'ReZero', 'pre', or 'post' (vanilla)\n",
    "nlayers = 6                     # the number of Layers\n",
    "lr = .04                        # Initial learning rate\n",
    "epochs = 3                      # The number of epochs\n",
    "emsize = 128                    # embedding dimension\n",
    "nhid = 256                      # the dimension of the feedforward network model\n",
    "nhead = 8                       # the number of heads in self attention\n",
    "dropout = 0.1                   # the dropout value\n",
    "\n",
    "setup_and_train(epochs, lr, emsize, nhid, nlayers, nhead, dropout, encoder_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 128 layer ReZero Transformer\n",
    "\n",
    "As promised in the title, we can use the `'ReZero'` architecture to train extremely deep Transformer networks. To render a `128` layer transformer tranable, we again reduce the learning rate (to the Adagrad default value of `lr = 0.01`).\n",
    "\n",
    "Training this 128 layer network takes about 20 minutes and after three epochs achieves the best validation ppl of around `168`. \n",
    "\n",
    "Unfortunately, it would require too much memory to quickly evaluate the input-output Jacobian for this deep network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 1192 batches | lr 0.01 | ms/batch   322 | loss  6.89 | ppl    985\n",
      "| epoch   1 |   400/ 1192 batches | lr 0.01 | ms/batch   319 | loss  5.84 | ppl    344\n",
      "| epoch   1 |   600/ 1192 batches | lr 0.01 | ms/batch   328 | loss  5.72 | ppl    304\n",
      "| epoch   1 |   800/ 1192 batches | lr 0.01 | ms/batch   318 | loss  5.68 | ppl    292\n",
      "| epoch   1 |  1000/ 1192 batches | lr 0.01 | ms/batch   324 | loss  5.59 | ppl    269\n",
      "----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 427.25s | valid loss  5.35 | valid ppl   211.38\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 1192 batches | lr 0.01 | ms/batch   320 | loss  5.16 | ppl    174\n",
      "| epoch   2 |   400/ 1192 batches | lr 0.01 | ms/batch   322 | loss  5.12 | ppl    167\n",
      "| epoch   2 |   600/ 1192 batches | lr 0.01 | ms/batch   321 | loss  5.11 | ppl    166\n",
      "| epoch   2 |   800/ 1192 batches | lr 0.01 | ms/batch   319 | loss  5.15 | ppl    173\n",
      "| epoch   2 |  1000/ 1192 batches | lr 0.01 | ms/batch   322 | loss  5.13 | ppl    170\n",
      "----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 422.47s | valid loss  5.19 | valid ppl   179.56\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 1192 batches | lr 0.01 | ms/batch   326 | loss  4.84 | ppl    126\n",
      "| epoch   3 |   400/ 1192 batches | lr 0.01 | ms/batch   320 | loss  4.83 | ppl    126\n",
      "| epoch   3 |   600/ 1192 batches | lr 0.01 | ms/batch   323 | loss  4.85 | ppl    128\n",
      "| epoch   3 |   800/ 1192 batches | lr 0.01 | ms/batch   321 | loss  4.92 | ppl    136\n",
      "| epoch   3 |  1000/ 1192 batches | lr 0.01 | ms/batch   320 | loss  4.91 | ppl    135\n",
      "----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 424.36s | valid loss  5.12 | valid ppl   167.97\n",
      "----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "# The model is set up with the hyperparameter below.\n",
    "\n",
    "encoder_version = 'ReZero'      # architecture: 'ReZero', 'pre', or 'post' (vanilla)\n",
    "nlayers = 128                   # the number of Layers\n",
    "lr = .01                        # Initial learning rate\n",
    "epochs = 3                      # The number of epochs\n",
    "emsize = 128                    # embedding dimension\n",
    "nhid = 256                      # the dimension of the feedforward network model\n",
    "nhead = 8                       # the number of heads in self attention\n",
    "dropout = 0.1                   # the dropout value\n",
    "\n",
    "setup_and_train(epochs, lr, emsize, nhid, nlayers, nhead, dropout, encoder_version, plt_jacobian = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
